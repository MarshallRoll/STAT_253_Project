---
title: "hw4"
author: "Marshall Roll"
date: "2022-11-01"
output: html_document
---

```{r}
## See answer to Q2 to explain irregularities

# library statements 
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
library(probably)
library(rpart.plot)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
# read in data

stroke <- read_csv("https://raw.githubusercontent.com/MarshallRoll/STAT_253_Project/main/healthcare-dataset-stroke-data.csv?token=GHSAT0AAAAAABYKVOXLQBJENJTQ27T53WIWY3BQ27Q")

```
```{r}
# data cleaning
stroke_clean <- stroke %>% 
  filter(bmi != "N/A") %>% 
  mutate(bmi = as.numeric(bmi)) %>% 
  filter(smoking_status != "Unknown") %>% 
  select(-id) %>% 
  mutate(stroke = relevel(factor(stroke), ref='0'))
```

Required Analysis

Start working on building a classification model to answer a research question on your data set. For HW4, only include your classification model work (leave your regression models work in another file).

For this homework,

Specify the research question for a classification task.

Try to implement at least 2 different classification methods to answer your research question.

Reflect on the information gained from these two methods and how you might justify this method to others.

Keep in mind that the final project will require you to complete the pieces below. Use this as a guide for your work but don’t try to accomplish everything for HW4:

## Answers to Questions: 

We will create a model to effectively answer the question, will someone have a stroke.

Our first classification method will be creating a logistic regression model using LASSO. 

```{r}
# creation of cv folds
stroke_cv <- vfold_cv(stroke_clean, v = 15)
```


```{r}
# Logistic LASSO Regression Model Spec
logistic_lasso_spec_tune <- logistic_reg() %>%
    set_engine('glmnet') %>%
    set_args(mixture = 1, penalty = tune()) %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(stroke~ ., data = stroke_clean) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_lasso_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-4, -1)), #log10 transformed  (kept moving min down from 0)
  levels = 100)

tune_output <- tune_grid( 
  log_lasso_wf, # workflow
  resamples = stroke_cv, # cv folds
  metrics = metric_set(roc_auc,accuracy),
  control = control_resamples(save_pred = TRUE, event_level = 'second'),
  grid = penalty_grid # penalty grid defined above
)
```
```{r}
autoplot(tune_output) + theme_classic()
```

```{r}
# Select Penalty
best_se_penalty <- select_by_one_std_err(tune_output, metric = 'roc_auc', desc(penalty)) # choose penalty value based on the largest penalty within 1 se of the highest CV roc_auc
best_se_penalty
```
```{r}
# Fit Final Model
final_fit_se <- finalize_workflow(log_lasso_wf, best_se_penalty) %>% # incorporates penalty value to workflow 
    fit(data = stroke_clean)

final_fit_se %>% tidy()
```
```{r}
glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    # Extract coefficient path (sorted from highest to lowest lambda)
    this_coeff_path <- bool_predictor_exclude[row,]
    # Compute and return the # of lambdas until this variable is out forever
    ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```
```{r}
# evalutation metrics 

# CV results for "best lambda"
tune_output %>%
    collect_metrics() %>%
    filter(penalty == best_se_penalty %>% pull(penalty))

stroke_clean %>%
    count(stroke)

# Compute the NIR
3246/(3246+180)
```


```{r}
final_output <- final_fit_se %>% predict(new_data = stroke_clean, type='prob') %>% bind_cols(stroke_clean)

final_output %>%
  ggplot(aes(x = stroke, y = .pred_1)) +
  geom_boxplot()
```
```{r}
# thresholds in terms of reference level
threshold_output <- final_output %>%
    threshold_perf(truth = stroke, estimate = .pred_0, thresholds = seq(0,1,by=.01)) 

# J-index v. threshold for not_spam
threshold_output %>%
    filter(.metric == 'j_index') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'J-index', x = 'threshold') +
    theme_classic()

threshold_output %>%
    filter(.metric == 'j_index') %>%
    arrange(desc(.estimate))

threshold_output %>%
    filter(.metric == 'distance') %>%
    arrange(.estimate)

```
```{r}
log_metrics <- metric_set(accuracy,sens,yardstick::spec)

final_output %>%
    mutate(.pred_class = make_two_class_pred(.pred_0, levels(stroke), threshold = .91)) %>%
    log_metrics(truth = stroke, estimate = .pred_class, event_level = 'second')
```
We choose this threshold in an effort tor prioritize a higher specificity in the context of the data. When using this model to predict whether or not someone will have a stroke, it is important that the number of false positives is higher than the number of false negatives. Meaning that we are more likely to over predict a stroke than tell someone who is going to have a stroke they are not going to.  

```{r}
set.seed(10)

ct_spec_tune <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = tune(),  
           min_n = 2, 
           tree_depth = NULL) %>% 
  set_mode('classification') 

data_rec <- recipe(stroke ~ ., data = stroke_clean)%>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

data_wf_tune <- workflow() %>%
  add_model(ct_spec_tune) %>%
  add_recipe(data_rec)

param_grid <- grid_regular(cost_complexity(range = c(-5, 1)), levels = 10) 

tune_res <- tune_grid(
  data_wf_tune, 
  resamples = stroke_cv, 
  grid = param_grid, 
  metrics = metric_set(accuracy) #change this for regression trees
)
autoplot(tune_res) + theme_classic()
```
```{r}
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)

stroke_clean_final_fit <- fit(data_wf_final, data = stroke_clean)


tune_res %>% 
  collect_metrics() %>%
  filter(cost_complexity == best_complexity %>% pull(cost_complexity))
```
```{r}
tree_mod_lowcp <- fit(
    data_wf_tune %>%
        update_model(ct_spec_tune %>% set_args(cost_complexity = .0001)),
    data = stroke_clean
)
tree_mod_highcp <- fit(
    data_wf_tune %>%
        update_model(ct_spec_tune %>% set_args(cost_complexity = .1)),
    data = stroke_clean
)

tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
```
```{r}
stroke_clean_final_fit %>% extract_fit_engine() %>% rpart.plot()
```
```{r}
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
```

Classification - Methods
Indicate at least 2 different methods used to answer your classification research question.
Describe what you did to evaluate the models explored.
Indicate how you estimated quantitative evaluation metrics.
Describe the goals / purpose of the methods used in the overall context of your research investigations.
Classification - Results
Summarize your final model and justify your model choice (see below for ways to justify your choice).
Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)
Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.

Classification - Conclusions - Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? - If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. - Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.